{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9O3aM3Tb28q",
        "outputId": "31c90604-20bb-4750-8f1a-f1b5a37110db"
      },
      "outputs": [],
      "source": [
        "!pip3 install 'torch'\n",
        "!pip3 install 'torchvision'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip3 install 'matplotlib'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-1            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-4  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 150     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 10       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 1000\n",
        "SCHEDULER_TYPE = 'LR'\n",
        "NORM_TYPE = 'GROUP'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "outputs": [],
      "source": [
        "# This dataset has 100 classes containing 600 images each. There are 500 training images and 100 testing images per \n",
        "# class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \n",
        "# \"coarse\" label (the superclass to which it belongs).\n",
        "# Define transforms for training phase\n",
        "\n",
        "# random crop, random horizontal flip, per-pixel normalization \n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(0.5), # 0.5 probability\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], # TODO: Check \n",
        "                                                           std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomCrop(32),\n",
        "                                      transforms.RandomHorizontalFlip(0.5), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                           std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfVq_uDHLbsR",
        "outputId": "d2f62627-f967-4a9a-987c-dff9c0ee0ae7"
      },
      "outputs": [],
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./CentralizedResNet'):\n",
        "  !git clone https://github.com/AML-SergioMejia/CentralizedResNet.git\n",
        "\n",
        "root_dir = \"CIFAR100\"\n",
        "\"\"\"\"\n",
        "root: str,\n",
        "train: bool = True, \n",
        "transform: Optional[Callable] = None, \n",
        "target_transform: Optional[Callable] = None, \n",
        "download: bool = False\n",
        "\"\"\"\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = torchvision.datasets.CIFAR100( root_dir, transform=train_transform, download=True)#Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100( root_dir, train=False, transform=train_transform, download=True)\n",
        "\n",
        "stratified_sampling = sklearn.model_selection.train_test_split([i for i in range(len(train_dataset))], test_size=0.2, stratify=train_dataset.targets)\n",
        "train_indexes = stratified_sampling[0] # split the indices for your train split\n",
        "val_indexes = stratified_sampling[1] # split the indices for your val split\n",
        "\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VriRw8SI1nle",
        "outputId": "65d06bd5-3f7f-4e64-dd26-e3de6902e80b"
      },
      "outputs": [],
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exHUjtXa22DN",
        "outputId": "2243694c-f963-4326-e942-a9a6818f5193"
      },
      "outputs": [],
      "source": [
        "from CentralizedResNet.resnet_cifar import ResNet20\n",
        "# We need 100 outputs\n",
        "net = ResNet20(num_blocks=3, num_classes=100, option='B', norm_type=NORM_TYPE)\n",
        "\n",
        "import os.path\n",
        "ckpt_path = 'trained_ResNet20'\n",
        "if SCHEDULER_TYPE is not None:\n",
        "  ckpt_path += f'_{SCHEDULER_TYPE}'\n",
        "ckpt_path += f'_{NORM_TYPE}'\n",
        "ckpt_path += '.pt'\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "  net.load_state_dict(torch.load(ckpt_path))\n",
        "  print(\"LOADED CHECKPOINT\")\n",
        "print(f\"Number of model parameters = {sum(p.numel() for p in net.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "if SCHEDULER_TYPE == 'WarmRestarts':\n",
        "  scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "elif SCHEDULER_TYPE == 'LR':\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.00001)\n",
        "else:\n",
        "  scheduler = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcoQ5fD49yT_",
        "outputId": "2206a92b-f665-46b4-a6da-cb006ad7afa3"
      },
      "outputs": [],
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{} [LR={}]'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  i = 0\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "    with torch.set_grad_enabled(True):\n",
        "      # Forward pass to the network\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Compute loss based on output and ground truth\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      if optimizer is not None:\n",
        "        optimizer.step() # update weights based on accumulated gradients\n",
        "    if scheduler is not None:\n",
        "      scheduler.step(epoch + i / len(train_dataloader))\n",
        "    i += 1\n",
        "\n",
        "    current_step += 1\n",
        "  # end for dataset iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Validation on last state of network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3HV5pqJg1o",
        "outputId": "4085e718-bcfa-4c5a-bbdb-aaedf545c964"
      },
      "outputs": [],
      "source": [
        "def validate_input(network, dataset):\n",
        "  network = network.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  network.train(False) # Set Network to evaluation mode\n",
        "  val_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  for input, labels in tqdm(val_dataloader):\n",
        "    input = input.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = network(input)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(dataset))\n",
        "  loss = running_loss / len(val_dataloader)\n",
        "  return accuracy, loss\n",
        "#accuracy = validate_input(net, val_dataset)\n",
        "#print('Validation Accuracy: {}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS4kCkdo15I4"
      },
      "source": [
        "**Training + validation per epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai8yPPw718NG"
      },
      "outputs": [],
      "source": [
        "# !! Call \"Prepare training\" cell again before executing this module !!\n",
        "\n",
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "loss_values = []\n",
        "acc_values = []\n",
        "\n",
        "last_val = 0\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_correct = 0\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    running_correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "  # end for dataset iteration\n",
        "  \n",
        "  accuracy_val, loss_val = validate_input(net, val_dataset)\n",
        "\n",
        "  # Save best model\n",
        "  if epoch > 50 and accuracy_val > last_val:\n",
        "    last_val = accuracy_val\n",
        "    path = 'trained_ResNet20'\n",
        "    if scheduler is not None:\n",
        "      path += f'_{SCHEDULER_TYPE}'\n",
        "    path += f'_{NORM_TYPE}'\n",
        "    torch.save(net.state_dict(), f'{path}.pt')\n",
        "  \n",
        "  loss_train = running_loss/len(train_dataloader.dataset)\n",
        "  accuracy_train = running_correct/len(train_dataloader.dataset)\n",
        "  \n",
        "  loss_values.append([loss_train, loss_val])\n",
        "  acc_values.append([accuracy_train, accuracy_val])\n",
        "  print('Validation Accuracy: {}'.format(accuracy_val))\n",
        "\n",
        "  # Step the scheduler\n",
        "  if scheduler is not None:\n",
        "    scheduler.step() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2AEelrML5gU"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(range(NUM_EPOCHS), loss_values)\n",
        "plt.legend([\"Train\", \"Val\"])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(NUM_EPOCHS), acc_values)\n",
        "plt.legend([\"Train\", \"Val\"])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSHcUqLB5yWO",
        "outputId": "fdcaa178-4b58-4c69-ad66-02cd975ab2ce"
      },
      "outputs": [],
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXI-Zch4VjkG"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPE8IwspVJ-1"
      },
      "outputs": [],
      "source": [
        "path = 'trained_ResNet20'\n",
        "if scheduler is not None:\n",
        "  path += f'_{SCHEDULER_TYPE}'\n",
        "path += f'_{NORM_TYPE}'\n",
        "torch.save(net.state_dict(), f'{path}.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "25b9d76d03052f04b853783704bf9224d9817fdb32617c036d1776cea483770c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
